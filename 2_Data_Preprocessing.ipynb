{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "## 1. Loading data\n",
    "There are several common datatypes: .h5 , .pkl , .json , .txt and compacted datatypes .z , .gz , .zip ,etc. <br>\n",
    "Generally, we tend to use .h5, because it takes up relatively small space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \" \"\n",
    "f = h5py.File(path, 'r')\n",
    "f.keys()                    # check keys in .h5 file, we need to read it by the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['f1','f2','f3']         \n",
    "labels = ['l1','l2']\n",
    "data = pd.read_hdf(path,key=\"The key you saw in the last step\",columns=features+labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of DGCNN, we are gonna use 7 features and 5 labels( labels depend on what task you are doing) as our input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Features(7) | Labels(5) |\n",
    "|:---|:--- |\n",
    "|\"j1_etarel\" -- delta eta, |'J_t',|\n",
    "|\"j1_phirel\" -- delta phi, |'J_q'|\n",
    "|\"log(j1_pt)\" -- log pt, |'J_g'|\n",
    "|\"log(j1_e)\" -- log E, |'J_w'|\n",
    "|\"log(j1_ptrel)\" -- log(pt / ptjet), |'J_z'|\n",
    "|\"log(j1_erel)\" -- log(E / Ejet), ||\n",
    "|\"j1_deltaR\" -- delta R||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j1_etarel: ration of the eta of each constituent to the eta of the jet<br>\n",
    "j1_phirel: ratio of the phi of each constituent to the phi of the jet<br>\n",
    "j1_pt: constituent pt (transverse momentum)<br>\n",
    "j1_e: constituent energy<br>\n",
    "j1_ptrel: ratio of the pT of each constituent to the pT of the jet<br>\n",
    "j1_erel: ration of the energy of each constituent of the energy of the jet<br>\n",
    "j1_deltaR: sqrt((Δeta)2 + (Δ phi)2 ) <br><br>\n",
    "j_g: gluon jet<br>\n",
    "j_q: quark jet <br>\n",
    "j_w: W boson jet <br>\n",
    "j_z: Z boson jet<br>\n",
    "j_t: Top jet<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise: \n",
    "Read out all the columns and try to understand what they are.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature construction\n",
    "We cannot get the log values directly from the original file, therefore a little feature construction is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feature[\"log(j1_pt)\"] = np.log(data_feature['j1_pt'])\n",
    "data_feature[\"log(j1_e)\"] = np.log(data_feature[\"j1_e\"])\n",
    "data_feature[\"log(j1_ptrel)\"] = np.log(data_feature['j1_ptrel'])\n",
    "data_feature[\"log(j1_erel)\"] = np.log(data_feature['j1_erel'])\n",
    "\n",
    "data_feature.drop(['j1_pt','j1_e','j1_ptrel','j1_erel'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's combine the features and labels so that we can send it to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.concat([data_feature,data_label],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Downsizing jets\n",
    "In the data we got, the number of constituents contained in each jet is different, ranging from 20 to 200. While we need a fixed size as input in the machine learning process, that is to say, we need to manually specify the number of constituents for each jet. If we set nConstituents = 40, all Jets whose number of constituents is less than 40 will be zero-padded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) How do we identify jets\n",
    "In the data I have contacted, there are two forms: particle-based and jet-based. <br>\n",
    "\n",
    "For the particle-based data, there should be a feature help identify the data. For example \"j_index\", it tells you the unique index of a jet. Get it <a href=\"https://drive.google.com/file/d/1DCpxWbWtqU4sQwmGbZTg-4cdGAWonDKy/view?usp=sharing\">here</a>.<br>\n",
    "\n",
    "For the jet-based data, each row represents a jet, you can get specific number of constinuents by conditional slicing. Get it <a href=\"https://zenodo.org/record/2603256#.X62WkFqSmbh\">here</a>.<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) N-Constituents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels+['j_index']\n",
    "data_label = pd.DataFrame(darray, columns=labels)\n",
    "data_all = pd.concat([data_feature, data_label],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def data_transform (nConstituents, data_all):\n",
    "    kColumns = data_all.columns.shape[0]\n",
    "\n",
    "    # we expect the output shape (mJets, nConstituents, kColumns)\n",
    "    jet_list = list(set(data_all['j_index']))\n",
    "    data_expected = []\n",
    "\n",
    "    for jet in tqdm(jet_list):\n",
    "        # Zero padding for insufficient jets. \n",
    "        # So we create a empty array and add signals in.\n",
    "        jet_frame = np.zeros((nConstituents, kColumns))\n",
    "        jet_temp = data_all[data_all['j_index']==jet].values\n",
    "        if (jet_temp.shape[0]<nConstituents):\n",
    "            for i, constituent in enumerate(jet_temp):\n",
    "                jet_frame[i] = constituent\n",
    "        else:\n",
    "            jet_frame += jet_temp[:nConstituents]\n",
    "        data_expected.append(jet_frame)\n",
    "\n",
    "    # \"j_index\" is useless for machine learning part. Drop it!\n",
    "    return np.array(data_expected[:,:,:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_transform(40, data_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not the only solution or the fastest function to accomplish the goal. You can try to develop a better one. If you find a better method, please share to your collegues. Because we are gonna use this method for almost all the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Excercise\n",
    "Try to think how you can get the same data shape with a jet-based data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Encoding\n",
    "Some times we need to encode some of the columns, for example, the label column come with value {0: Light Jet, 4: Charm Jet, 5: Bottom Jet}. we need to map it to a 3-columns data, each column represents a kind of jet. It is called One-Hot Encoding. <br>\n",
    "You should not run the following code because the data we are using do not require encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.get_dummies(df['The_Label_Column'],prefix='label')\n",
    "# You may want to rename the columns.\n",
    "label_df.rename(columns={'flavor_0':\"Light_Jet\",'flavor_4':'Charm_Jet','flavor_5':'Bottom_Jet'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just in case you want to transform it back. You can multiply the weight for each column and sum it to get back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_prev = np.sum(label_encoded*[0,4,5],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise\n",
    "Create an 200-length array with random integer ranging \\[0,5\\], Encode it and then get it back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Test Split\n",
    "We rely on the sklearn package to accomplish it. There is a build-in function.<br>\n",
    "Choose a random seed and use it for all your researches. Wanna know why? To keep Consistent input very time you run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise\n",
    "Apply the code above to your data. For further explanations for parameters, Google it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
